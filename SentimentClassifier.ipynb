{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Project: Climate Change - Sentiment Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Topics discussed:\n",
    "- Using the manually labelled text. Developed Sentiment Classifier for predicting Sentiment of new sentence\n",
    "- Tried different classification Models given below\n",
    "-      K-NN\n",
    "-      Logistic Regression\n",
    "-      Multinomial NB\n",
    "-      Linear Support Vector Machine\n",
    "-      Kernelized Support Vector Machines\n",
    "-      Neural Network\n",
    "-      Neural Network (tuned parameters)\n",
    "-      XG Boost Classifier\n",
    "-      Decision Tree \n",
    "-      Random Forest\n",
    "- Comparison Of All models\n",
    "- Identified the best model and tested example sentences "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reading in the combined 2000 rows with target variable"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Sentiment file\n",
    "sentiment_file = \"_sentiments_target.csv\"\n",
    "df = pd.read_csv(sentiment_file, sep=\"\\t\")"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking the sample entries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "df.head()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>It also increases carbon dioxide emissions whi...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We can already see this happening.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The ecological disaster is a consequence of no...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We may be dealing with an issue with a level o...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Preventable chronic diseases are Australias le...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence sentiment\n",
       "0  It also increases carbon dioxide emissions whi...   neutral\n",
       "1                 We can already see this happening.  negative\n",
       "2  The ecological disaster is a consequence of no...  positive\n",
       "3  We may be dealing with an issue with a level o...  negative\n",
       "4  Preventable chronic diseases are Australias le...  negative"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Checking the label distribution \n",
    "#### Seems like most values are categorized as neutral"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "df.sentiment.value_counts()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "neutral     1148\n",
       "negative     662\n",
       "positive     462\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "vectorizer = TfidfVectorizer(use_idf=True, norm=\"l2\", stop_words=\"english\", max_df=0.7)\n",
    "X = vectorizer.fit_transform(df.sentence)\n",
    "y = df.sentiment"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "X.shape, y.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((2272, 7011), (2272,))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Spliting data into test and training sets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### User defined function to retrieve model score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def get_score(model,X_train, y_train, cv):\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv)\n",
    "    return scores "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### User defined function to return best parameter with max score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def print_compare_score(val,scores,score_max,param,param_best):\n",
    "    print(\"{} = {}: {}\\n{:.3f}, {:.3f}\\n\".format(val,param, scores, scores.mean(), scores.std()))\n",
    "    if scores.mean() > score_max:\n",
    "        score_max = scores.mean()\n",
    "        param_best = param \n",
    "    return(score_max,param_best)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### User defined function to fit test data to learned model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def train_test(X_train, X_test, y_train, y_test, classifier):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    \n",
    "    print(\"Train score: {:.2f}\".format(classifier.score(X_train, y_train)))\n",
    "    print(\"Test score: {:.2f}\\n\".format(classifier.score(X_test, y_test)))\n",
    "    print(\"Classification report:\\n{}\".format(classification_report(y_test, pred, zero_division=0)))\n",
    "    print(confusion_matrix(y_test,pred))\n",
    "    \n",
    "    return classifier"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-NN"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Finding the Best value of K"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "score_max = 0                      # Score_max is a temoporay variable to store the max score \n",
    "param_best = 0\n",
    "for param in [10,40,50,70,100]:\n",
    "    model = KNeighborsClassifier(n_neighbors=param)\n",
    "    scores = get_score(model,X_train, y_train, 5)\n",
    "    (score_max,param_best) = print_compare_score('k',scores,score_max,param,param_best)        \n",
    "print(\"Highest score : {:.3f} when k = {}\".format(score_max, param_best))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "k = 10: [0.49725275 0.48901099 0.51239669 0.46831956 0.47933884]\n",
      "0.489, 0.015\n",
      "\n",
      "k = 40: [0.51098901 0.50824176 0.52892562 0.49586777 0.49586777]\n",
      "0.508, 0.012\n",
      "\n",
      "k = 50: [0.50549451 0.50824176 0.52892562 0.50137741 0.51790634]\n",
      "0.512, 0.010\n",
      "\n",
      "k = 70: [0.52197802 0.51648352 0.52066116 0.51790634 0.50688705]\n",
      "0.517, 0.005\n",
      "\n",
      "k = 100: [0.51098901 0.5        0.51790634 0.50137741 0.51790634]\n",
      "0.510, 0.008\n",
      "\n",
      "Highest score : 0.517 when k = 70\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the Model to see the Test and Train Score with k = 50"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "print(\"k = {}\".format(param_best))\n",
    "knn = KNeighborsClassifier(n_neighbors=param_best)\n",
    "knn = train_test(X_train, X_test, y_train, y_test, knn)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "k = 70\n",
      "Train score: 0.53\n",
      "Test score: 0.50\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.53      0.11      0.18       150\n",
      "     neutral       0.49      0.95      0.65       220\n",
      "    positive       0.50      0.01      0.02        85\n",
      "\n",
      "    accuracy                           0.50       455\n",
      "   macro avg       0.51      0.36      0.28       455\n",
      "weighted avg       0.51      0.50      0.38       455\n",
      "\n",
      "[[ 16 133   1]\n",
      " [ 11 209   0]\n",
      " [  3  81   1]]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "summary_test = {}\n",
    "summary_train = {}\n",
    "summary_test[\"k-NNs Test\"] = round(knn.score(X_test, y_test), 3)\n",
    "summary_train[\"k-NNs Train\"] = round(knn.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic Regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "lr = LogisticRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "scores = get_score(lr,X_train, y_train, 5)\n",
    "print(\"{}\\n{:.3f}, {:.3f}\".format(scores, scores.mean(), scores.std()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.52747253 0.51098901 0.5261708  0.51515152 0.51515152]\n",
      "0.519, 0.007\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the Model to see the Test and Train Score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "lr = train_test(X_train, X_test, y_train, y_test, lr)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train score: 0.83\n",
      "Test score: 0.48\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.13      0.21       150\n",
      "     neutral       0.49      0.90      0.64       220\n",
      "    positive       0.14      0.01      0.02        85\n",
      "\n",
      "    accuracy                           0.48       455\n",
      "   macro avg       0.37      0.35      0.29       455\n",
      "weighted avg       0.42      0.48      0.38       455\n",
      "\n",
      "[[ 20 127   3]\n",
      " [ 18 199   3]\n",
      " [  4  80   1]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "summary_test[\"Logistic Regression Test\"] = round(lr.score(X_test, y_test), 3)\n",
    "summary_train[\"Logistic Regression Train\"] = round(lr.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Multinomial NB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "mnb = MultinomialNB()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "scores = get_score(mnb,X_train, y_train, 5)\n",
    "print(\"{}\\n{:.3f}, {:.3f}\".format(scores, scores.mean(), scores.std()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.51648352 0.52472527 0.51790634 0.51515152 0.51790634]\n",
      "0.518, 0.003\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the Model to see the Test and Train Score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "mnb = train_test(X_train, X_test, y_train, y_test, mnb)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train score: 0.71\n",
      "Test score: 0.49\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.04      0.07       150\n",
      "     neutral       0.49      0.99      0.66       220\n",
      "    positive       0.00      0.00      0.00        85\n",
      "\n",
      "    accuracy                           0.49       455\n",
      "   macro avg       0.35      0.34      0.24       455\n",
      "weighted avg       0.42      0.49      0.34       455\n",
      "\n",
      "[[  6 143   1]\n",
      " [  2 218   0]\n",
      " [  3  82   0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "summary_test[\"Multinomial Naive Bayes Test\"] = round(mnb.score(X_test, y_test), 3)\n",
    "summary_train[\"Multinomial Naive Bayes Train\"] = round(mnb.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling with Linear Support Vector Machines (SVMs)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Finding the Best value of C"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "score_max = 0\n",
    "param_best = 0\n",
    "for param in [0.01, 0.03, 0.1, 0.3, 0.4, 0.5, 1, 3, 10]:\n",
    "    model = LinearSVC(C=param)\n",
    "    scores = get_score(model,X_train, y_train, 5)\n",
    "    (score_max,param_best) = print_compare_score('C',scores,score_max,param,param_best) \n",
    "        \n",
    "print(\"Highest score : {:.3f} when C = {}\".format(score_max, param_best))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C = 0.01: [0.51098901 0.51098901 0.51239669 0.50964187 0.50964187]\n",
      "0.511, 0.001\n",
      "\n",
      "C = 0.03: [0.51098901 0.51098901 0.51239669 0.50964187 0.50964187]\n",
      "0.511, 0.001\n",
      "\n",
      "C = 0.1: [0.52197802 0.50824176 0.50964187 0.51239669 0.51790634]\n",
      "0.514, 0.005\n",
      "\n",
      "C = 0.3: [0.51923077 0.50274725 0.53443526 0.52066116 0.49862259]\n",
      "0.515, 0.013\n",
      "\n",
      "C = 0.4: [0.51648352 0.48351648 0.53719008 0.50964187 0.49862259]\n",
      "0.509, 0.018\n",
      "\n",
      "C = 0.5: [0.51648352 0.48626374 0.53168044 0.49862259 0.49586777]\n",
      "0.506, 0.016\n",
      "\n",
      "C = 1: [0.48901099 0.46428571 0.50137741 0.47658402 0.49862259]\n",
      "0.486, 0.014\n",
      "\n",
      "C = 3: [0.47527473 0.43956044 0.49586777 0.45730028 0.49035813]\n",
      "0.472, 0.021\n",
      "\n",
      "C = 10: [0.46153846 0.4478022  0.49035813 0.44352617 0.48209366]\n",
      "0.465, 0.018\n",
      "\n",
      "Highest score : 0.515 when C = 0.3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the Model to see the Test and Train Score with C = 0.3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "print(\"C = {}\".format(param_best))\n",
    "svm = LinearSVC(C=param_best)\n",
    "svm = train_test(X_train, X_test, y_train, y_test, svm)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C = 0.3\n",
      "Train score: 0.96\n",
      "Test score: 0.49\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.49      0.22      0.30       150\n",
      "     neutral       0.50      0.85      0.63       220\n",
      "    positive       0.14      0.02      0.04        85\n",
      "\n",
      "    accuracy                           0.49       455\n",
      "   macro avg       0.38      0.36      0.32       455\n",
      "weighted avg       0.43      0.49      0.41       455\n",
      "\n",
      "[[ 33 112   5]\n",
      " [ 27 186   7]\n",
      " [  7  76   2]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "summary_test[\"Linear SVMs Test\"] = round(svm.score(X_test, y_test), 3)\n",
    "summary_train[\"Linear SVMs Train\"] = round(svm.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling with Kernelized Support Vector Machines (KSVMs)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Finding the Best value of C"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "score_max = 0\n",
    "param_best = 0\n",
    "for param in [0.01, 0.03, 0.1, 0.3, 1, 3, 10]:\n",
    "    model = SVC(C=param, kernel=\"rbf\", gamma=\"scale\")\n",
    "    scores = get_score(model,X_train, y_train, 5)\n",
    "    (score_max,param_best) = print_compare_score('C',scores,score_max,param,param_best) \n",
    "        \n",
    "print(\"Highest score : {:.3f} when C = {}\".format(score_max, param_best))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C = 0.01: [0.51098901 0.51098901 0.51239669 0.50964187 0.50964187]\n",
      "0.511, 0.001\n",
      "\n",
      "C = 0.03: [0.51098901 0.51098901 0.51239669 0.50964187 0.50964187]\n",
      "0.511, 0.001\n",
      "\n",
      "C = 0.1: [0.51098901 0.51098901 0.51239669 0.50964187 0.50964187]\n",
      "0.511, 0.001\n",
      "\n",
      "C = 0.3: [0.51098901 0.51098901 0.51239669 0.50964187 0.50964187]\n",
      "0.511, 0.001\n",
      "\n",
      "C = 1: [0.51373626 0.51373626 0.51515152 0.50964187 0.50688705]\n",
      "0.512, 0.003\n",
      "\n",
      "C = 3: [0.52747253 0.5        0.52892562 0.51790634 0.49586777]\n",
      "0.514, 0.014\n",
      "\n",
      "C = 10: [0.52747253 0.5        0.52892562 0.51790634 0.49586777]\n",
      "0.514, 0.014\n",
      "\n",
      "Highest score : 0.514 when C = 3\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Run the Model to see the Test and Train Score with C = 3"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "print(\"C = {}\".format(param_best))\n",
    "ksvm = SVC(C = param_best, kernel = \"rbf\", gamma = \"scale\")\n",
    "ksvm = train_test(X_train, X_test, y_train, y_test, ksvm)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C = 3\n",
      "Train score: 1.00\n",
      "Test score: 0.48\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.44      0.16      0.24       150\n",
      "     neutral       0.49      0.87      0.63       220\n",
      "    positive       0.09      0.01      0.02        85\n",
      "\n",
      "    accuracy                           0.48       455\n",
      "   macro avg       0.34      0.35      0.30       455\n",
      "weighted avg       0.40      0.48      0.39       455\n",
      "\n",
      "[[ 24 121   5]\n",
      " [ 23 192   5]\n",
      " [  7  77   1]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "summary_test[\"Kernelized SVMs Test\"] = round(ksvm.score(X_test, y_test), 3)\n",
    "summary_train[\"Kernelized SVMs Train\"] = round(ksvm.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modeling with Neural Networks"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Finding the best hidden_layer_sizes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "score_max = 0\n",
    "param_best = 0\n",
    "for param in [10, 30, 100]:\n",
    "    model = MLPClassifier(hidden_layer_sizes=(param, ), max_iter=2000, activation=\"relu\", random_state=0)\n",
    "    scores = get_score(model,X_train, y_train, 5)\n",
    "    (score_max,param_best) = print_compare_score('hidden_layer_size',scores,score_max,param,param_best) \n",
    "        \n",
    "print(\"Highest score : {:.3f} when hidden_layer_sizes = {}\".format(score_max, param_best))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hidden_layer_size = 10: [0.47252747 0.46153846 0.46280992 0.44077135 0.4738292 ]\n",
      "0.462, 0.012\n",
      "\n",
      "hidden_layer_size = 30: [0.45604396 0.46153846 0.4738292  0.42975207 0.4600551 ]\n",
      "0.456, 0.015\n",
      "\n",
      "hidden_layer_size = 100: [0.46978022 0.46153846 0.46556474 0.4214876  0.4600551 ]\n",
      "0.456, 0.017\n",
      "\n",
      "Highest score : 0.462 when hidden_layer_sizes = 10\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the Model to see the Test and Train Score with hidden_layer_sizes = 30"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "print(\"hidden_layer_size = {}\".format(param_best))\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(param_best, ), max_iter=2000, random_state=0) # default activation is 'relu'\n",
    "mlp = train_test(X_train, X_test, y_train, y_test, mlp)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "hidden_layer_size = 10\n",
      "Train score: 1.00\n",
      "Test score: 0.48\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.48      0.39      0.43       150\n",
      "     neutral       0.53      0.66      0.58       220\n",
      "    positive       0.26      0.18      0.21        85\n",
      "\n",
      "    accuracy                           0.48       455\n",
      "   macro avg       0.42      0.41      0.41       455\n",
      "weighted avg       0.46      0.48      0.47       455\n",
      "\n",
      "[[ 59  76  15]\n",
      " [ 48 145  27]\n",
      " [ 15  55  15]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "summary_test[\"Neural Networks Test\"] = round(mlp.score(X_test, y_test), 3)\n",
    "summary_train[\"Neural Networks Train\"] = round(mlp.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## NN with different solver that supports multi-labeled classification better"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "mlp_2 = MLPClassifier(hidden_layer_sizes=(param_best, ), max_iter=2000, solver = 'lbfgs', random_state=0) # default activation is 'relu' and default solver is adam\n",
    "scores = get_score(mlp_2,X_train, y_train, 5)\n",
    "score_max = scores.mean()\n",
    "print(\"Score : {:.3f} when hidden_layer_sizes = {}\\n\".format(score_max, param_best))\n",
    "mlp_2 = train_test(X_train, X_test, y_train, y_test, mlp_2)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score : 0.476 when hidden_layer_sizes = 10\n",
      "\n",
      "Train score: 1.00\n",
      "Test score: 0.52\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.51      0.39      0.45       150\n",
      "     neutral       0.55      0.71      0.62       220\n",
      "    positive       0.33      0.22      0.27        85\n",
      "\n",
      "    accuracy                           0.52       455\n",
      "   macro avg       0.47      0.44      0.45       455\n",
      "weighted avg       0.50      0.52      0.50       455\n",
      "\n",
      "[[ 59  72  19]\n",
      " [ 44 157  19]\n",
      " [ 12  54  19]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "source": [
    "summary_test[\"Neural Networks Test - Solver lbfgs \"] = round(mlp_2.score(X_test, y_test), 3)\n",
    "summary_train[\"Neural Networks Train - Solver lbfgs\"] = round(mlp_2.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "source": [
    "mlp_3 = MLPClassifier(hidden_layer_sizes=(param_best, ), max_iter=2000, activation = 'tanh', solver = 'sgd', random_state=0) \n",
    "# default activation is 'relu' and default solver is 'adam'\n",
    "scores = get_score(mlp_3,X_train, y_train, 5)\n",
    "score_max = scores.mean()\n",
    "print(\"Score : {:.3f} when hidden_layer_sizes = {}\\n\".format(score_max, param_best))\n",
    "mlp_3 = train_test(X_train, X_test, y_train, y_test, mlp_3)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score : 0.511 when hidden_layer_sizes = 10\n",
      "\n",
      "Train score: 0.51\n",
      "Test score: 0.48\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       150\n",
      "     neutral       0.48      1.00      0.65       220\n",
      "    positive       0.00      0.00      0.00        85\n",
      "\n",
      "    accuracy                           0.48       455\n",
      "   macro avg       0.16      0.33      0.22       455\n",
      "weighted avg       0.23      0.48      0.32       455\n",
      "\n",
      "[[  0 150   0]\n",
      " [  0 220   0]\n",
      " [  0  85   0]]\n"
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "summary_test[\"Neural Networks - tanh activation Test\"] = round(mlp_3.score(X_test, y_test), 3)\n",
    "summary_train[\"Neural Networks - tanh activation Train\"] = round(mlp_3.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Among the Neural Networks hidden_layer_sizes = 30 gave the best reults when we use tanh activation and sgd solver"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## XG boost classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "clf = GradientBoostingClassifier(n_estimators=300, learning_rate=0.1, max_depth=20, subsample = 0.5, random_state=0)\n",
    "scores = get_score(clf,X_train, y_train, 5)\n",
    "score_max = scores.mean()\n",
    "print(\"Score : {:.3f} when hidden_layer_sizes = {}\\n\".format(score_max, param_best))\n",
    "clf = train_test(X_train, X_test, y_train, y_test, clf)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Score : 0.493 when hidden_layer_sizes = 10\n",
      "\n",
      "Train score: 1.00\n",
      "Test score: 0.49\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.45      0.28      0.34       150\n",
      "     neutral       0.54      0.73      0.62       220\n",
      "    positive       0.30      0.22      0.26        85\n",
      "\n",
      "    accuracy                           0.49       455\n",
      "   macro avg       0.43      0.41      0.41       455\n",
      "weighted avg       0.46      0.49      0.46       455\n",
      "\n",
      "[[ 42  85  23]\n",
      " [ 37 161  22]\n",
      " [ 15  51  19]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "summary_test[\"XG Boost Test\"] = round(clf.score(X_test, y_test), 3)\n",
    "summary_train[\"XG Boost Train\"] = round(clf.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decision Tree Classifier"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "dt = tree.DecisionTreeClassifier()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "scores = get_score(dt,X_train, y_train, 5)\n",
    "print(\"{}\\n{:.3f}, {:.3f}\".format(scores, scores.mean(), scores.std()))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.46978022 0.47802198 0.4600551  0.42699725 0.49035813]\n",
      "0.465, 0.021\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the Model to see the Test and Train Score"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "source": [
    "dt = train_test(X_train, X_test, y_train, y_test, dt)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train score: 1.00\n",
      "Test score: 0.45\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.42      0.25      0.31       150\n",
      "     neutral       0.53      0.66      0.59       220\n",
      "    positive       0.25      0.27      0.26        85\n",
      "\n",
      "    accuracy                           0.45       455\n",
      "   macro avg       0.40      0.39      0.39       455\n",
      "weighted avg       0.44      0.45      0.43       455\n",
      "\n",
      "[[ 37  83  30]\n",
      " [ 37 145  38]\n",
      " [ 15  47  23]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "source": [
    "summary_test[\"Decision Tree Test\"] = round(dt.score(X_test, y_test), 3)\n",
    "summary_train[\"Decision Tree Train\"] = round(dt.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Random Forest"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Trying different max_depth "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "score_max = 0\n",
    "param_best = 0\n",
    "for param in [10,18, 20, 30, 40]:\n",
    "    model = RandomForestClassifier(max_depth=param)\n",
    "    scores = get_score(model,X_train, y_train, 5)\n",
    "    (score_max,param_best) = print_compare_score('C',scores,score_max,param,param_best) \n",
    "        \n",
    "print(\"Highest score : {:.3f} when C = {}\".format(score_max, param_best))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C = 10: [0.51098901 0.51098901 0.51239669 0.50964187 0.50964187]\n",
      "0.511, 0.001\n",
      "\n",
      "C = 18: [0.51098901 0.51098901 0.51239669 0.50964187 0.51515152]\n",
      "0.512, 0.002\n",
      "\n",
      "C = 20: [0.51098901 0.51098901 0.51515152 0.50964187 0.51239669]\n",
      "0.512, 0.002\n",
      "\n",
      "C = 30: [0.51648352 0.51098901 0.51239669 0.50688705 0.51239669]\n",
      "0.512, 0.003\n",
      "\n",
      "C = 40: [0.51648352 0.50549451 0.51515152 0.50413223 0.51515152]\n",
      "0.511, 0.005\n",
      "\n",
      "Highest score : 0.512 when C = 18\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Run the Model to see the Test and Train Score with max_depth = 30"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "source": [
    "print(\"C = {}\".format(param_best))\n",
    "rf = RandomForestClassifier(max_depth=param_best)\n",
    "rf = train_test(X_train, X_test, y_train, y_test, rf)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "C = 18\n",
      "Train score: 0.53\n",
      "Test score: 0.48\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00       150\n",
      "     neutral       0.48      1.00      0.65       220\n",
      "    positive       0.00      0.00      0.00        85\n",
      "\n",
      "    accuracy                           0.48       455\n",
      "   macro avg       0.16      0.33      0.22       455\n",
      "weighted avg       0.23      0.48      0.32       455\n",
      "\n",
      "[[  0 150   0]\n",
      " [  0 220   0]\n",
      " [  0  85   0]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "summary_test[\"Random Forest Test\"] = round(rf.score(X_test, y_test), 3)\n",
    "summary_train[\"Random Forest Train\"] = round(rf.score(X_train, y_train), 3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## The Neural Network with the lbfgs solver has the best performance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "summary_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'k-NNs Test': 0.497,\n",
       " 'Logistic Regression Test': 0.484,\n",
       " 'Multinomial Naive Bayes Test': 0.492,\n",
       " 'Linear SVMs Test': 0.486,\n",
       " 'Kernelized SVMs Test': 0.477,\n",
       " 'Neural Networks Test': 0.481,\n",
       " 'Neural Networks Test - Solver lbfgs ': 0.516,\n",
       " 'Neural Networks - tanh activation Test': 0.484,\n",
       " 'XG Boost Test': 0.488,\n",
       " 'Decision Tree Test': 0.451,\n",
       " 'Random Forest Test': 0.484}"
      ]
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## New sentences"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "# grab some sentences from the larger file, not the 2000 that we already classified\n",
    "text1 = \"This is amazing, climate change initiatives have created so many jobs!\" # Positive\n",
    "text2 = \"I hate the bad idea of hotter temperatures and the horrible fact that ice caps are melting\" # Negative\n",
    "text3 = \"Ice caps are melting faster each year\" # Neutral\n",
    "text4 = \"Climate change is fake news, this is the coldest winter ever\" # Negative\n",
    "text5 = \"Hubspot makes my day a lot easier, this makes me super happy! :)\" # Positive\n",
    "text6 = \"Your customer service is a nightmare! Totally useless!!\"   # Negative\n",
    "text7 = \"The older interface was much simpler\" # Negative\n",
    "text8 = \"Awful experience. I would never buy this product again!\" # Negative\n",
    "text9 = \"I don't think there is anything I really dislike about the product\"  # Neutral\n",
    "text10 = \"I love how Zapier takes different apps and ties them together, perfect idea!\" # Positive\n",
    "text11 = \"I still need to further test Zapier to say if its useful for me or not\"  # Neutral\n",
    "text12 = \"Zapier is sooooo confusing to me\" # Negative"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "new_texts = [text1, text2, text3, text4,text5,text6,text7,text8,text9,text10,text11,text12]\n",
    "X_new = vectorizer.transform(new_texts)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "source": [
    "knn.predict(X_new)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['neutral', 'neutral', 'neutral', 'neutral', 'neutral', 'neutral',\n",
       "       'neutral', 'neutral', 'negative', 'neutral', 'neutral', 'neutral'],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "mlp.predict(X_new)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['positive', 'negative', 'neutral', 'neutral', 'neutral',\n",
       "       'positive', 'neutral', 'negative', 'negative', 'neutral',\n",
       "       'neutral', 'neutral'], dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 48
    }
   ],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "source": [
    "mlp_2.predict(X_new)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array(['positive', 'neutral', 'neutral', 'neutral', 'negative', 'neutral',\n",
       "       'neutral', 'negative', 'negative', 'neutral', 'neutral', 'neutral'],\n",
       "      dtype='<U8')"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Conclusion"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As we have see the results from all the different models Neural Networks Test - Solver lbfgs gave us highest test score of 0.492. Which is very much expected here as 'lbfgs' solver is real good with amount of data we have here. Some other models gave us good results on the test data were XG boost and Linear SVMs"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}